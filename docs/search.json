[
  {
    "objectID": "Research Question 2.html",
    "href": "Research Question 2.html",
    "title": "Research Question 2",
    "section": "",
    "text": "Research Question 2:\nHow do factors like high-caloric food intake, smoking habits, vegetable intake and technology dependency correlate with weight?\nMethodology:\nThe above research question can be solved by using multilinear regression and variable selection.Adjusted R-square values is used as a criterion to perform the variable selection.\nThe statistical analysis of this dataset involves several steps:\n\nData Loading:\n\nFirstly,we need to load the CSV file.\n\n\nShow the code\nlibrary(ggplot2)\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nShow the code\nlibrary(car)\n\n\nWarning: package 'car' was built under R version 4.3.3\n\n\nLoading required package: carData\n\n\nWarning: package 'carData' was built under R version 4.3.3\n\n\nShow the code\nlibrary(caret)\n\n\nWarning: package 'caret' was built under R version 4.3.3\n\n\nLoading required package: lattice\n\n\nShow the code\nlibrary(MASS)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:MASS':\n\n    select\n\n\nThe following object is masked from 'package:car':\n\n    recode\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nShow the code\ndata &lt;- read.csv(\"C:\\\\Users\\\\arava\\\\OneDrive\\\\Desktop\\\\STAT515\\\\modified_obesity_data.csv\")\n\n\n\nData Preprocessing:\n\nThis step involves data cleaning where rows with missing values are deleted and categorical variables are converted into dummy numerical variables.\n\n\nShow the code\ndata &lt;- na.omit(data)\ndata$Highcaloric_food &lt;- as.numeric(data$Highcaloric_food == \"yes\")\ndata$Smoking_Habit &lt;- as.numeric(data$Smoking_Habit == \"yes\")\n\n\n\nModel Building:\n\nWe build a linear model with Highcaloric_food, Smoking_Habit, Vegetable_intake and Technology_dependency as predictors and Weight as the dependent variable.\n\n\nShow the code\nlm_model &lt;- lm(Weight ~ Highcaloric_food + Vegetable_intake + Smoking_Habit + Technology_dependency, data = data)\n\n\n\n\nShow the code\nstepwise_adj_r2 &lt;- function(model, data) {\n  current_adj_r2 &lt;- summary(model)$adj.r.squared\n  predictors &lt;- names(coef(model))\n  best_model &lt;- model\n  improvement &lt;- TRUE\n  \n  while (improvement) {\n    improvement &lt;- FALSE\n    best_adj_r2 &lt;- current_adj_r2\n    \n    for (predictor in predictors[-1]) { \n      formula &lt;- as.formula(paste(\"Weight ~\", paste(setdiff(predictors[-1], predictor), collapse = \" + \")))\n      candidate_model &lt;- lm(formula, data = data)\n      candidate_adj_r2 &lt;- summary(candidate_model)$adj.r.squared\n      \n      if (candidate_adj_r2 &gt; best_adj_r2) {\n        best_adj_r2 &lt;- candidate_adj_r2\n        best_model &lt;- candidate_model\n        improvement &lt;- TRUE\n      }\n    }\n    \n    if (improvement) {\n      current_adj_r2 &lt;- best_adj_r2\n      predictors &lt;- names(coef(best_model))\n    }\n  }\n  \n  return(best_model)\n}\n\n\n\nVariable Selection:\n\nAfter building  a model the next step is to perform Stepwise regression which is a variable selection technique by taking Adjusted R-squared values as a criterion to improve the model fitting.\n\n\nShow the code\nfinal_model &lt;- stepwise_adj_r2(lm_model, data)\n\n\n\nDisplaying the Outputs and Plots:\n\n After performing stepwise selection the summary of the final model is printed which gives the coefficients, significance levels, and statistics like R-squared values and F-statistics.We also print the coefficient plot, residual plot and variable Importance plot.\n\n\nShow the code\nsummary_final_model &lt;- summary(final_model)\nprint(summary_final_model)\n\n\n\nCall:\nlm(formula = Weight ~ Highcaloric_food + Vegetable_intake + Smoking_Habit + \n    Technology_dependency, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-55.505 -17.647   0.165  15.725  78.498 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            42.3267     3.0838  13.726  &lt; 2e-16 ***\nHighcaloric_food       23.1905     1.7077  13.580  &lt; 2e-16 ***\nVegetable_intake       10.6626     1.0326  10.326  &lt; 2e-16 ***\nSmoking_Habit           8.3548     4.0203   2.078 0.037820 *  \nTechnology_dependency  -3.0029     0.9027  -3.327 0.000895 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.54 on 2053 degrees of freedom\nMultiple R-squared:  0.1283,    Adjusted R-squared:  0.1266 \nF-statistic: 75.55 on 4 and 2053 DF,  p-value: &lt; 2.2e-16\n\n\nSummary of the model:\n\nResiduals:\n\nResiduals are the differences between the observed values of the dependent variable (Weight) and the values predicted by the model.\nMin\nThe model underpredicted this observation by 55.505 units, given the smallest residual as -55.505.\nFirst Quartile\nFor a quarter of the data, the model’s predictions are around 17.647 units too high, as indicated by 25% of the residuals being smaller than -17.647.\nMedian\nThe median residual is very close to 0 (0.165), indicating that the model’s predictions are generally accurate, as half of the residuals are below this value and half are above.\nThird Quartile\nMost observations have the model’s prediction within about 15.725 units of the actual values, as indicated by the 75% of residuals being smaller than 15.725.\nMax\nThe greatest residual, which is 78.498 units, indicates that the highest underprediction of the model for any given observation is 78.498 units.\n\nCoefficients:\n\nThe predictors’ coefficients indicate the expected rise in the dependent variable (weight) for each unit increase in the predictor, assuming no change in the other predictors.\nIntercept\nThe expected weight is about 42.33 units when all predictors are zero.\nHighcaloric_food\nIn comparison to not consuming high-calorie food, there is an average weight rise of 23.19 units linked to high-calorie food consumption\nVegetable_intake\nWeight increases by around 10.66 units for every unit increase in vegetable intake.\nSmoking_Habit\nThe average weight rise of smokers  is 8.35 units higher than that of non-smokers.\nTechnology_dependency\nWeight drops by 3.00 units for every unit increase in technological dependency.\n\nStatistical Significance(Pr(&gt;|t|)):\nIt provides the p values to determine whether the model is statistically significant or not.Significance level(alpha) is generally set to 0.05. The null hypothesis states that no effect or no difference between groups whereas the alternative hypothesis stating that there is a  difference between groups.If the p&lt; alpha(0.05)  the model is considered statistically significant, and the null hypothesis is rejected in favor of the alternative hypothesis.From the summary we can see that p values of all the predictors &lt; 0.05 therefore suggesting that the model is statistically significant.\nResidual Standard Error:\nThis indicates how much the expected values differ from the actual values on an average.The deviation of the model is around 24.54 weight units.\nMultiple R-squared (0.1283) and Adjusted R-squared (0.1266):\nThe model accounts for approximately 12.83% of the variance observed in the dependent variable, weight.When several predictors are present, the adjusted measure becomes more accurate since it accounts for approximately 12.66% of the variation in the model.\nF-statistic and its p-value:\n\nThis tests whether at least one predictor variable has a non-zero coefficient or not. The less p values states that the model is statistically significant.\nIn conclusion we can say that the model is statistically significant but with an Adjusted R-squared value of 0.1266 we can say that there is a small variance in the weight.\n\n\nShow the code\n# Coefficient plot\ncoef_plot_adj_r2 &lt;- ggplot(data = as.data.frame(coef(summary(final_model))), aes(x = rownames(coef(summary(final_model))), y = Estimate)) +\n  geom_bar(stat = \"identity\", fill = \"orange\", color = \"black\") +\n  labs(title = \"Coefficients of Predictors (Adjusted R-Squared Criterion)\", x = \"Predictors\", y = \"Coefficient Estimate\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\nprint(coef_plot_adj_r2)\n\n\n\n\n\nThe above bar chart displays the coefficient estimates for each predictor in the regression model.\nImplications of the Coefficients:\nPositive coefficients(Highcaloric_food, Vegetable_intake, Smoking_Habit) shows a direct relationship with weight, where increases in these variables are associated with increases in weight.\nNegative coefficient (Technology_dependency) shows an inverse relationship, where increases in this variable are associated with decreases in weight.\nConclusion:\nThe strength and influence of each predictor on the dependent variable weight, are shown by the relative sizes of the bars. Highcaloric_food shows the most dominant effect, followed by vegetable_intake suggesting that these variables are the most important contributors to the difference in weight shown in your data. The figure clearly illustrates the various effects of dietary and lifestyle choices on weight and offers a visual representation of the regression results that were previously presented.\n\n\nShow the code\n# Residual plot\nresidual_plot_adj_r2 &lt;- ggplot(data, aes(x = fitted(final_model), y = residuals(final_model))) +\n  geom_point(color = \"black\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Residual Plot (Adjusted R-Squared Criterion)\", x = \"Fitted Values\", y = \"Residuals\") +\n  theme_minimal()\nprint(residual_plot_adj_r2)\n\n\n\n\n\nThe residual shows the residuals on the vertical axis (the differences between observed and predicted values) plotted against the fitted values(predicted) on the horizontal axis.The dashed red line at zero indicates perfect prediction by the model.The residual plot shows residuals scattered randomly around zero, mostly on the positive side. However, distinct clusters around certain predictions, such as 80 and 100, suggest possible missing nonlinear effects or interactions in the model.Extreme outliers, both above and below the zero line, require investigation for potential data errors.Clear patterns, such as vertical clustering, indicate potential issues like incorrect assumptions about variable relationships or overrepresentation of predictor levels in certain ranges.\nIn conclusion,the residual plot is a tool for diagnostic checking. It helps in assessing whether the assumptions of linear regression (linearity, independence, constant varience, and normality of residuals) are reasonably satisfied or if further model improvements are needed or not.\n\n\nShow the code\n# Variable importance plot\nvar_importance_adj_r2 &lt;- ggplot(data = as.data.frame(varImp(final_model)), aes(x = rownames(varImp(final_model)), y = Overall)) +\n  geom_bar(stat = \"identity\", fill = \"cornsilk\", color = \"black\") +\n  labs(title = \"Variable Importance\", x = \"Predictors\", y = \"Importance\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\nprint(var_importance_adj_r2)\n\n\n\n\n\nThe Variable Importance bar chart visually represents the relative importance of different predictors in the regression model. Highcaloric_food predictor has the highest importance in the model, indicating that it has a potential effect on the dependent variable (weight). This aligns with the earlier regression output where the coefficient for high-caloric food intake was significantly positive, suggesting a strong association with increased weight. Smoking_habit shows the least importance compared to all the predictors. This lesser importance reflects its relatively lower coefficient and it was also marginally significant in the regression analysis (p-value close to the typical significance threshold of 0.05).\nIn conclusion,this plot ranks the predictors by their influence on the model’s ability to predict weight.Predictors with higher bars are more critical for the model, meaning changes in these predictors are associated with more significant changes in weight."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "The purpose of redesigning bad graphs is to improve clarity, accuracy, and effectiveness in conveying information. Bad graphs often suffer from issues such as cluttered visuals, misleading representations, inappropriate use of chart types, or inadequate labeling. Redesigning these graphs aims to address these shortcomings, making the data easier to interpret, enhancing communication, and ultimately leading to better-informed decision-making.\nReference for the 1st Visualization:\nHow Big is Apple? This Visualization Puts Things Into Perspective. (2019, July 3). HowMuch. https://howmuch.net/articles/putting-apple-into-perspective\nThe chart compares the market capitalization of multiple entities. Market capitalization is the total value of a company’s outstanding shares of stock. One entity (likely Apple) has the highest market capitalization and serves as the reference point for comparison. Other entities could be companies from different sectors, countries’ GDPs, or other relevant categories\nReasons for revamping the dataset Limited Data Points: This type of chart works well for comparing a few entities, but if you have many companies or variables, the chart can become cluttered and difficult to read. The long horizontal bars can take up a lot of space, making it hard to compare all the data points effectively.\nFocus on Ranking, Not Magnitude: While the chart shows the relative order of market capitalization, it doesn’t directly display the actual values for each entity. This can make it difficult to judge the exact difference between companies.\nLimited Ability to Show Trends: This visualization is primarily for static comparisons. It’s not ideal for showing trends over time or relationships between variables.\nReference for the 2nd Visualization:\nTop 10 most expensive riots in the U.S. insurance history. (2021, April 20). HowMuch. https://howmuch.net/articles/top-10-americas-most-destructive-riots-of-all-time"
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "library(plotly)\n\nLoading required package: ggplot2\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata &lt;- data.frame(\n  variable = c(\"Apple\", \"Africa's 25 GDP\", \"Netherlands GDP\", \"Google\", \"U.S. Dept. of Defence's Budget (2019)\",\n               \"Coca-Cola\", \"Phone Carriers\", \"NASA's Budget since 1998\", \"NASA\",\n               \"Alibaba\", \"All Cryptocurrencies\", \"Nike+Adidas\", \"Top 300 Movies\", \"Bitcoin\", \"General Motors\",'Narcos wealth'),\n  value = c(886, 839, 831, 751, 686, 678, 559, 451, 450, 416, 233, 182, 138, 138, 51,44),\n  color = c(\"red\", \"orange\", \"green\", \"blue\", \"purple\", \"brown\", \"pink\", \"grey\", \"black\", \"yellow\",\"gold\", \"darkgreen\", \"violet\", \"darkred\", \"skyblue\",\"darkblue\")\n)\n\ndata &lt;- data %&gt;% arrange(value)\n\n\np &lt;- ggplot(data, aes(x = reorder(variable, value), y = value, fill = variable)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Market Capitalization of several Entities\",\n       x = \"Variables\",\n       y = \"Value (in Billions)\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nggplotly(p)"
  },
  {
    "objectID": "code.html#interpretation",
    "href": "code.html#interpretation",
    "title": "Code",
    "section": "Interpretation:",
    "text": "Interpretation:\nThe bar chart compares the market capitalization of various entities against a reference point, likely Apple, which has the highest market capitalization. Here’s a breakdown of the interpretation:\nReference Point: Apple likely has the highest market capitalization among the entities compared in the chart, serving as the reference point for comparison. Its market capitalization is represented by the tallest bar on the chart. Entities Compared: Other entities in the chart represent a diverse range of categories, such as companies from different sectors, countries’ GDPs, or other relevant categories. These entities are ranked based on their market capitalization values, with the lowest being the “Narcos wealth” at 44 billion USD. Comparison with Apple: Alibaba: The market capitalization of Alibaba is mentioned to be nearly half of Apple’s. This indicates that while Alibaba is a significant entity, its market capitalization is substantially lower compared to Apple. Other Entities: The chart allows for the comparison of market capitalizations of various entities relative to Apple. Entities with higher market capitalizations will have taller bars, while those with lower market capitalizations will have shorter bars. Insights from the Chart: The chart provides insights into the relative market capitalizations of different entities compared to Apple. It highlights the dominance of Apple in terms of market capitalization compared to other entities. It also showcases the wide range of market capitalizations across different entities, reflecting the diversity of the global economy. Overall, the chart effectively visualizes the market capitalizations of multiple entities, with Apple as the reference point for comparison, providing valuable insights into the relative sizes of different entities in the market.\n\nlibrary(plotly)\n\ndata &lt;- data.frame(\n  variable = c(\"Apple\", \"Africa's 25 GDP\", \"Netherlands GDP\", \"Google\", \"U.S. Dept. of Defence's Budget (2019)\",\n               \"Coca-Cola\", \"Phone Carriers\", \"NASA's Budget since 1998\", \"NASA\",\n               \"Alibaba\", \"All Cryptocurrencies\", \"Nike+Adidas\", \"Top 300 Movies\", \"Bitcoin\", \"General Motors\",'Narcos wealth'),\n  value = c(886, 839, 831, 751, 686, 678, 559, 451, 450, 416, 233, 182, 138, 138, 51,44),\n  color = c(\"red\", \"orange\", \"green\", \"blue\", \"purple\", \"brown\", \"pink\", \"grey\", \"black\", \"yellow\",\n            \"gold\", \"darkgreen\", \"violet\", \"darkred\", \"skyblue\",\"darkblue\")\n)\n\ndata &lt;- data[order(data$value), ]\n\np &lt;- plot_ly(data, \n             type = 'scatterpolar', \n             theta = data$variable, \n             r = data$value,\n             fill = 'toself',\n             line = list(color = data$color),\n             marker = list(color = data$color)) %&gt;%\n  layout(title = list(text = \"Market Capitalization of several sectors against apple\", \n                      x = 0.5,  \n                      y = 0.95,  \n                      xanchor = \"center\", \n                      yanchor = \"top\"),  \n         polar = list(radialaxis = list(visible = TRUE, title = 'Value (in Billions)')),\n         margin = list(t = 100))  \n\np\n\nNo scatterpolar mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -&gt; https://plotly.com/r/reference/#scatter-mode\n\n\nA line object has been specified, but lines is not in the mode\nAdding lines to the mode..."
  },
  {
    "objectID": "code.html#interpretation-1",
    "href": "code.html#interpretation-1",
    "title": "Code",
    "section": "Interpretation:",
    "text": "Interpretation:\nThe radar chart compares the market capitalization of various entities against a reference point, likely Apple, which has the highest market capitalization. Here’s a breakdown of the interpretation:\nReference Point: Apple likely has the highest market capitalization among the entities compared in the chart, serving as the reference point for comparison. Its market capitalization is represented by the tallest bar on the chart. Entities Compared: Other entities in the chart represent a diverse range of categories, such as companies from different sectors, countries’ GDPs, or other relevant categories. These entities are ranked based on their market capitalization values, with the lowest being the “Narcos wealth” at 44 billion USD. Comparison with Apple: Alibaba: The market capitalization of Alibaba is mentioned to be nearly half of Apple’s. This indicates that while Alibaba is a significant entity, its market capitalization is substantially lower compared to Apple. Other Entities: The chart allows for the comparison of market capitalizations of various entities relative to Apple. Entities with higher market capitalizations will have taller bars, while those with lower market capitalizations will have shorter bars. Insights from the Chart: The chart provides insights into the relative market capitalizations of different entities compared to Apple. It highlights the dominance of Apple in terms of market capitalization compared to other entities. It also showcases the wide range of market capitalizations across different entities, reflecting the diversity of the global economy. Overall, the chart effectively visualizes the market capitalizations of multiple entities, with Apple as the reference point for comparison, providing valuable insights into the relative sizes of different entities in the market.\n\n\nlibrary(plotly)\n\ndata &lt;- data.frame(\n  Date = as.Date(c(\"1992-04-29\", \"2021-01-06\", \"1965-08-11\", \"1967-07-23\", \"1980-05-17\",\n                   \"1968-04-04\", \"1977-07-13\", \"1967-07-12\", \"1968-04-06\", \"1968-04-04\")),\n  Location = c(\"Los Angeles, CA\", \"Washington, DC\", \"Los Angeles, CA\", \"Detroit, MI\", \"Miami, FL\",\n               \"Washington, DC\", \"New York, NY\", \"Newark, NJ\", \"Baltimore, MD\", \"Chicago, IL\"),\n  Insurance_Loss = c(1.42e9, 5e8, 3.57e8, 3.22e8, 2.04e8, 1.79e8, 1.18e8, 1.15e8, 1.04e8, 9.7e7)\n)\n\n\ndata$Insurance_Loss_Millions &lt;- data$Insurance_Loss / 1e6\n\n\ndata$Insurance_Loss_Millions_Formatted &lt;- format(data$Insurance_Loss_Millions, digits = 10, nsmall = 2)\n\n\nplot &lt;- plot_ly(data, x = ~Date, y = ~Insurance_Loss_Millions, type = 'scatter', mode = 'lines+markers',\n                text = ~paste(\"Location: \", Location, \"&lt;br&gt;Insurance Loss: $\", Insurance_Loss_Millions_Formatted, \" Million\"),\n                hoverinfo = 'text') %&gt;%\n  layout(title = \"Insurance Loss Over Time Period\",\n         xaxis = list(title = \"Date\"),\n         yaxis = list(title = \"Insurance Loss (Millions $)\"),\n         plot_bgcolor = \"#f8f9fa\", \n         paper_bgcolor = \"#f8f9fa\")  \n\n\nplot"
  },
  {
    "objectID": "code.html#interpretation-2",
    "href": "code.html#interpretation-2",
    "title": "Code",
    "section": "Interpretation:",
    "text": "Interpretation:\nAn individual insurance loss incident is represented by each point in the interactive visualization, which shows the pattern of insurance losses across time. Each event’s date is represented by the x-coordinate, and the loss amount expressed in millions of dollars is shown by the y-coordinate respectively. Through the in-depth review of the data, hovering over individual points provides further information like the precise location and amount of the insurance loss. Through the process of plot analysis, interested parties can recognize trends, pinpoint important incidents, and understand the extent and kind of losses. This understanding supports the development of insurance policies, resource allocation, and risk assessment—all of which are crucial risk management methods. All things considered, the visual aid is a useful means of concluding past insurance loss data that can be applied to help stakeholders create better plans for risk management and reaction to unforeseen circumstances.\n\n\nlibrary(plotly)\nlibrary(dplyr)\n\ndata &lt;- data.frame(\n  Date = as.Date(c(\"1992-04-29\", \"2021-01-06\", \"1965-08-11\", \"1967-07-23\", \"1980-05-17\",\n                   \"1968-04-04\", \"1977-07-13\", \"1967-07-12\", \"1968-04-06\", \"1968-04-04\")),\n  Location = c(\"Los Angeles, CA\", \"Washington, DC\", \"Los Angeles, CA\", \"Detroit, MI\", \"Miami, FL\",\n               \"Washington, DC\", \"New York, NY\", \"Newark, NJ\", \"Baltimore, MD\", \"Chicago, IL\"),\n  Insurance_Loss = c(1.42e9, 5e8, 3.57e8, 3.22e8, 2.04e8, 1.79e8, 1.18e8, 1.15e8, 1.04e8, 9.7e7),\n  Lat = c(34.052235, 38.89511, 34.052235, 42.331429, 25.761681,\n          38.89511, 40.712776, 40.735657, 39.290386, 41.878113),\n  Lon = c(-118.243683, -77.03637, -118.243683, -83.045753, -80.191788,\n          -77.03637, -74.005974, -74.172366, -76.61219, -87.629799)\n)\n\ndata_summarized &lt;- data %&gt;%\n  group_by(Location, Lat, Lon) %&gt;%\n  summarize(Total_Insurance_Loss = sum(Insurance_Loss))\n\n`summarise()` has grouped output by 'Location', 'Lat'. You can override using\nthe `.groups` argument.\n\nmap_plot &lt;- plot_ly(data_summarized, type = 'scattermapbox', mode = 'markers+text',\n                    lat = ~Lat, lon = ~Lon,\n                    text = ~paste(\"Location: \", Location, \"&lt;br&gt;Insurance Loss: $\", \n                                  format(Total_Insurance_Loss / 1e6, digits = 10, nsmall = 2),\n                                  \" million\"),\n                    hoverinfo = 'text') %&gt;%\n  layout(title = \"Total Insurance Loss by Location (in millions) between the years 1965 to 2001\",\n         mapbox = list(\n           style = \"open-street-map\",  \n           zoom = 3,  \n           center = list(lat = 38, lon = -97)  \n         )) %&gt;%\n  add_annotations(\n    text = \"Time Frame\",\n    x = 0.03,\n    y = 0.95,\n    xref = \"paper\",\n    yref = \"paper\",\n    showarrow = FALSE\n  )\n\nmap_plot"
  },
  {
    "objectID": "code.html#interpretation-3",
    "href": "code.html#interpretation-3",
    "title": "Code",
    "section": "Interpretation:",
    "text": "Interpretation:\nData on insurance losses from 1965 to 2001 is presented in an informative manner in the following visualization. It displays marks on a map, each of which denotes an insurance loss location. At a given place, the size of each marker represents the overall insurance loss; larger markers denote higher losses. When viewers hover over a marker, important details like the name of the site and the overall insurance loss in millions of dollars are revealed, enabling them to comprehend the size and spatial distribution of losses in various areas. Viewers can find trends indicating recurrent incidents or locations prone to particular sorts of disasters by analyzing the map and identifying geographic areas with higher insurance losses.Users can zoom in, pan around the map, and acquire a deeper understanding of the spatial patterns and temporal trends of insurance losses with this interactive visualization that facilitates dynamic exploration. In the end, the visualization aids in risk assessment, strategic planning to lessen the impact of future insurance catastrophes, and informed decision-making for stakeholders in the insurance business, risk management, and disaster preparedness."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello everyone,\nMy name is Satya Jyothiraditya Arava and you can call me Aditya. I am currently pursuing my Masters in Data Analytics Engineering from George Mason University. I completed my graduation degree in Computer Science Engineering from Amrita School of Engineering,Bengaluru.\nDuring my term as a program analyst trainee I had the opportunity to get my hands on various technologies like Angular,React,Spring Boot,SQL and GCP.I am now mapping out my academic objectives, with which I can graduate from Mason with immense knowledge, so that I can boom in data analysis and its related technologies.To achieve my goals and aspirations I am enrolling in some interesting courses,including one on statistics with a concentration in R programming and another on data management and mining.\nI am looking forward to explore more technologies during my masters tenure."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aditya Arava",
    "section": "",
    "text": "My name is Satya Jyothiraditya Arava and you can call me Aditya. I am currently pursuing my Masters in Data Analytics Engineering from George Mason University. I completed my graduation degree in Computer Science Engineering from Amrita School of Engineering,Bengaluru.\nDuring my term as a program analyst trainee I had the opportunity to get my hands on various technologies like Angular,React,Spring Boot,SQL and GCP.I am now mapping out my academic objectives, with which I can graduate from Mason with immense knowledge, so that I can boom in data analysis and its related technologies.To achieve my goals and aspirations I am enrolling in some interesting courses,including one on statistics with a concentration in R programming and another on data management and mining.\nI am looking forward to explore more technologies during my masters tenure."
  },
  {
    "objectID": "Research Question 1.html",
    "href": "Research Question 1.html",
    "title": "Obesity Project",
    "section": "",
    "text": "Introduction:\nThis dataset include data for the estimation of obesity levels in individuals from the countries of Mexico, Peru and Colombia, based on their eating habits and physical condition. The data contains 17 attributes and 2111 records, the records are labeled with the class variable NObesity (Obesity Level), that allows classification of the data using the values of Insufficient Weight, Normal Weight, Overweight Level I, Overweight Level II, Obesity Type I, Obesity Type II and Obesity Type III. 77% of the data was generated synthetically using the Weka tool and the SMOTE filter, 23% of the data was collected directly from users through a web platform.\nDataset:\nUCI Machine Learning Repository. (n.d.). https://archive.ics.uci.edu/dataset/544/estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition\n\nResearch Question 1)\nIs there any significant relationship between family history with overweight(heredity) and the obesity levels of individuals?\nA)The question arose from the consideration of hereditary traits influencing individual health outcomes, particularly those related to familial predispositions and parental influences. Consequently, the objective is to conduct an analysis delineating the association between familial history of overweight conditions and the corresponding levels of obesity.\nModeling and Approach:\nDecision Tree: Decision trees are intuitive and easy to interpret, making them suitable for exploring relationships between categorical variables like family history with overweight and obesity levels. They can handle both categorical and numerical data, allowing to include various features from dataset in the analysis. Decision trees can reveal complex interactions and patterns in the data, helping one identify which factors are most influential in determining obesity levels based on family history.\nChi-Square Test: The chi-square test is commonly used to determine whether there is a significant association between two categorical variables. In our case, we can use the chi-square test to assess the independence between family history with overweight and obesity levels. If there is a significant association, it suggests that family history is a factor contributing to obesity levels. This test provides a statistical measure of the strength of the relationship between the variables, complementing the insights gained from the decision tree analysis.\nComprehensive Analysis: By combining both approaches, one can gain a comprehensive understanding of the relationship between family history with overweight and obesity levels. The decision tree analysis can provide insights into the hierarchical structure of the relationship and identify specific pathways leading to different obesity levels. The chi-square test offers a statistical validation of the observed relationship, enhancing the robustness of our findings.\n\n\nShow the code\nlibrary(caret)\n\n\nWarning: package 'caret' was built under R version 4.3.3\n\n\nLoading required package: ggplot2\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nLoading required package: lattice\n\n\nShow the code\nlibrary(MASS)\nlibrary(leaps)\n\n\nWarning: package 'leaps' was built under R version 4.3.3\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(GGally)\n\n\nWarning: package 'GGally' was built under R version 4.3.3\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\nShow the code\nlibrary(psych)\n\n\nWarning: package 'psych' was built under R version 4.3.3\n\n\n\nAttaching package: 'psych'\n\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\n\nShow the code\nlibrary(car)\n\n\nWarning: package 'car' was built under R version 4.3.3\n\n\nLoading required package: carData\n\n\nWarning: package 'carData' was built under R version 4.3.3\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:psych':\n\n    logit\n\n\nShow the code\nlibrary(tidyr)\nlibrary(gridExtra)\n\n\nWarning: package 'gridExtra' was built under R version 4.3.3\n\n\nShow the code\nlibrary(rpart)\n\n\nWarning: package 'rpart' was built under R version 4.3.3\n\n\nShow the code\nlibrary(tree)\n\n\nWarning: package 'tree' was built under R version 4.3.3\n\n\nShow the code\nlibrary(rpart.plot)\n\n\nWarning: package 'rpart.plot' was built under R version 4.3.3\n\n\nShow the code\nlibrary(caret)\nlibrary(plotly)\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:MASS':\n\n    select\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\nShow the code\nlibrary(vcd)\n\n\nWarning: package 'vcd' was built under R version 4.3.3\n\n\nLoading required package: grid\n\n\nShow the code\nobesity=read.csv(\"C:\\\\Users\\\\arava\\\\OneDrive\\\\Desktop\\\\STAT515\\\\modified_obesity_data.csv\")\ncolnames(obesity)\n\n\n [1] \"Gender\"                         \"Age\"                           \n [3] \"Height\"                         \"Weight\"                        \n [5] \"family_history_with_overweight\" \"Highcaloric_food\"              \n [7] \"Vegetable_intake\"               \"Main_Meal_Intake\"              \n [9] \"Food_between_meals\"             \"Smoking_Habit\"                 \n[11] \"Water_intake\"                   \"Calorie_Intake\"                \n[13] \"PHYSICAL_Activity\"              \"Technology_dependency\"         \n[15] \"Alcohol_Consumption\"            \"Mode_of_Transportation\"        \n[17] \"Obesity_Levels\"                \n\n\nDecision Tree Approach\n1. Handling of Categorical Variables Intuitive Handling: Decision trees inherently handle categorical variables very well. They can split nodes on such variables to assess their impact on outcomes (in this case, obesity levels). This means that family history, a categorical variable, can be directly used to split the data, providing clear insights into how it correlates with different categories of obesity. Direct Visualization: Each branch of the tree can be traced back to a decision based on family history, making it visually intuitive to see how significant family history is in predicting obesity levels.\n2. Capability to Model Non-linear Relationships Flexibility in Modeling: Decision trees do not assume linearity in the data, which is ideal for complex relationships that may exist between hereditary factors and obesity levels. They can model complex hierarchical relationships that other linear models might fail to capture. Interaction Effects: Decision trees can naturally model interactions between multiple variables without the need for explicit specification. This allows for the exploration of how family history interacts with other factors like diet, exercise, and age in influencing obesity.\nIn summary, using a decision tree to study the impact of family history on obesity levels leverages the model’s ability to handle categorical data, model complex relationships, and provide interpretable results that are crucial for both understanding and acting on the data’s insights.\n\n\nShow the code\nset.seed(123)\ntrain_indices &lt;- sample(nrow(obesity), nrow(obesity) * 0.8)\ntrain_data &lt;- obesity[train_indices, ]\ntest_data &lt;- obesity[-train_indices, ]\n\n\nThe utilization of the sample() function is employed in a random manner for the purpose of selecting indices designated for the training set. The proportion of the training set is established at 80% of the overall data. Subsequently, the training and testing sets are generated by utilizing the aforementioned selected indices.\nThe rpart() function is implemented to construct a regression tree model that is fitted to the training data. The formulation family_history_with_overweight ~ . delineates that the variable denoted as “family_history with_overweight” functions as the response variable, with all other variables within the dataset serving as predictors. To visually represent the fitted regression tree model, the rpart.plot() function is utilized.\n\n\nShow the code\nmodel &lt;- rpart(family_history_with_overweight ~ ., data = train_data)\nrpart.plot(model)\n\n\n\n\n\nInterpretation of Decision Tree:\nThis decision tree is focusing on the relationships between various physical measurements and lifestyle choices (like alcohol consumption and dietary habits) to assess obesity and other related conditions, with (“family_history_with_overweight” as main variable) The nodes show the probability of an individual falling into certain categories based on their characteristics, with deeper branches offering more specific conclusions based on additional factors.\nPearson’s chi-squared test :\nThis test is a statistical test applied to sets of categorical data to evaluate how likely it is that any observed difference between the sets arose by chance. It is the most widely used of many chi-squared tests (e.g., Yates, likelihood ratio, portmanteau test in time series, etc.) – statistical procedures whose results are evaluated by reference to the chi-squared distribution.\nThe chi-squared test can then be used to statistically confirm this relationship by testing the independence between these two categorical variables across the entire dataset.\nThe chi-squared test quantitatively confirms whether this influence is statistically significant, providing a robustness check to the observational patterns noted in the decision tree.\n\n\nShow the code\nobesity$family_history_with_overweight &lt;- as.factor(obesity$family_history_with_overweight)\nobesity$Obesity_Levels &lt;- as.factor(obesity$Obesity_Levels)\n\n# Create a contingency table\ncontingency_table &lt;- table(obesity$family_history_with_overweight, obesity$Obesity_Levels)\n\n# Perform chi-square test\nchi_sq_test &lt;- chisq.test(contingency_table)\n\n# Print the results\nprint(chi_sq_test)\n\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 618.04, df = 6, p-value &lt; 2.2e-16\n\n\nResults and Interpretation of Chi-squared test:\nThe result of Pearson’s chi-squared test indicates a highly significant association between the variables family_history_with_overweight and NObeyesdad i.e Obesity levels. Here’s the interpretation:\n\nX-squared: This is the test statistic of the chi-squared test. In this case, it is 621.98.\ndf (degrees of freedom): This represents the degrees of freedom associated with the chi-squared distribution. It is calculated as (number of rows - 1) * (number of columns - 1). In this case, df = 6.\np-value: This is the probability of observing a test statistic as extreme as the one calculated from the sample data, under the assumption that the null hypothesis is true. A p-value below a certain significance level (commonly 0.05) indicates that the association between the variables is statistically significant.\n\nIn this result: - The p-value is less than 2.2e-16, which is essentially zero. This indicates strong evidence against the null hypothesis. - Therefore, we reject the null hypothesis and conclude that there is a significant association between family_history_with_overweight and NObeyesdad.\nIn simpler terms, it suggests that there is a relationship between family history of overweight and the obesity status categories in the dataset.\nThe result of Pearson’s chi-squared test indicates a very strong association between the variables family_history_with_overweight and NObeyesdad i.e obesity levels. This means that there is a significant relationship between having a family history of overweight and the obesity status categories in the dataset.\nNow that it is clearly evident that there’s a strong relationship between two variables. lets look at the details.\n\n\nShow the code\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:gridExtra':\n\n    combine\n\n\nThe following object is masked from 'package:car':\n\n    recode\n\n\nThe following object is masked from 'package:MASS':\n\n    select\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nShow the code\nobesity_distribution &lt;- obesity %&gt;%\n  group_by(family_history_with_overweight, Obesity_Levels) %&gt;%\n  summarise(count = dplyr::n(), .groups = 'drop')\n\n\n# View the table to ensure correctness\nprint(obesity_distribution)\n\n\n# A tibble: 13 × 3\n   family_history_with_overweight Obesity_Levels      count\n   &lt;fct&gt;                          &lt;fct&gt;               &lt;int&gt;\n 1 no                             Insufficient_Weight   144\n 2 no                             Normal_Weight         127\n 3 no                             Obesity_Type_I          7\n 4 no                             Obesity_Type_II         1\n 5 no                             Overweight_Level_I     78\n 6 no                             Overweight_Level_II    16\n 7 yes                            Insufficient_Weight   124\n 8 yes                            Normal_Weight         145\n 9 yes                            Obesity_Type_I        332\n10 yes                            Obesity_Type_II       295\n11 yes                            Obesity_Type_III      324\n12 yes                            Overweight_Level_I    204\n13 yes                            Overweight_Level_II   261\n\n\n\n\nShow the code\nlibrary(plotly)\n\n# Plotting with Plotly Express\nplot_ly(data = obesity_distribution, x = ~Obesity_Levels, y = ~count, color = ~family_history_with_overweight,\n        type = \"bar\", barmode = \"group\") %&gt;%\n  layout(title = \"Distribution of Obesity Levels by Family History of Overweight\",\n         xaxis = list(title = \"Obesity Levels\"),\n         yaxis = list(title = \"Count\"),\n         legend = list(title = \"Family History: Overweight\")) %&gt;%\n  layout(legend = list(orientation = \"h\", x = 0.5, y = -0.15)) %&gt;%\n  layout(font = list(size = 11, family = \"Arial\")) # Adjust font size and family\n\n\nWarning in RColorBrewer::brewer.pal(N, \"Set2\"): minimal value for n is 3, returning requested palette with 3 different levels\nWarning in RColorBrewer::brewer.pal(N, \"Set2\"): minimal value for n is 3, returning requested palette with 3 different levels\n\n\nWarning: 'bar' objects don't have these attributes: 'barmode'\nValid attributes include:\n'_deprecated', 'alignmentgroup', 'base', 'basesrc', 'cliponaxis', 'constraintext', 'customdata', 'customdatasrc', 'dx', 'dy', 'error_x', 'error_y', 'hoverinfo', 'hoverinfosrc', 'hoverlabel', 'hovertemplate', 'hovertemplatesrc', 'hovertext', 'hovertextsrc', 'ids', 'idssrc', 'insidetextanchor', 'insidetextfont', 'legendgroup', 'legendgrouptitle', 'legendrank', 'marker', 'meta', 'metasrc', 'name', 'offset', 'offsetgroup', 'offsetsrc', 'opacity', 'orientation', 'outsidetextfont', 'selected', 'selectedpoints', 'showlegend', 'stream', 'text', 'textangle', 'textfont', 'textposition', 'textpositionsrc', 'textsrc', 'texttemplate', 'texttemplatesrc', 'transforms', 'type', 'uid', 'uirevision', 'unselected', 'visible', 'width', 'widthsrc', 'x', 'x0', 'xaxis', 'xcalendar', 'xhoverformat', 'xperiod', 'xperiod0', 'xperiodalignment', 'xsrc', 'y', 'y0', 'yaxis', 'ycalendar', 'yhoverformat', 'yperiod', 'yperiod0', 'yperiodalignment', 'ysrc', 'key', 'set', 'frame', 'transforms', '_isNestedKey', '_isSimpleKey', '_isGraticule', '_bbox'\nWarning: 'bar' objects don't have these attributes: 'barmode'\nValid attributes include:\n'_deprecated', 'alignmentgroup', 'base', 'basesrc', 'cliponaxis', 'constraintext', 'customdata', 'customdatasrc', 'dx', 'dy', 'error_x', 'error_y', 'hoverinfo', 'hoverinfosrc', 'hoverlabel', 'hovertemplate', 'hovertemplatesrc', 'hovertext', 'hovertextsrc', 'ids', 'idssrc', 'insidetextanchor', 'insidetextfont', 'legendgroup', 'legendgrouptitle', 'legendrank', 'marker', 'meta', 'metasrc', 'name', 'offset', 'offsetgroup', 'offsetsrc', 'opacity', 'orientation', 'outsidetextfont', 'selected', 'selectedpoints', 'showlegend', 'stream', 'text', 'textangle', 'textfont', 'textposition', 'textpositionsrc', 'textsrc', 'texttemplate', 'texttemplatesrc', 'transforms', 'type', 'uid', 'uirevision', 'unselected', 'visible', 'width', 'widthsrc', 'x', 'x0', 'xaxis', 'xcalendar', 'xhoverformat', 'xperiod', 'xperiod0', 'xperiodalignment', 'xsrc', 'y', 'y0', 'yaxis', 'ycalendar', 'yhoverformat', 'yperiod', 'yperiod0', 'yperiodalignment', 'ysrc', 'key', 'set', 'frame', 'transforms', '_isNestedKey', '_isSimpleKey', '_isGraticule', '_bbox'\n\n\n\n\n\n\nResults and Interpretation:\nThe graphical representation in the form of a bar plot illustrates the distribution of obesity levels across different categories, with obesity types and overweight levels delineated on the x-axis and corresponding counts depicted on the y-axis. A discernible trend emerges wherein obesity types 1, 2, and 3, alongside the overweight categories, exhibit notably higher frequencies. This pattern strongly suggests a substantive association between family history with overweight and the observed obesity levels. Consequently, it is substantiated that familial predispositions towards overweight conditions significantly contribute to the prevailing obesity levels among individuals."
  },
  {
    "objectID": "Research Question 3.html",
    "href": "Research Question 3.html",
    "title": "Research Question 3",
    "section": "",
    "text": "Research Question 3:\nIs there a distinct pattern or clustering of individuals based on their health-related attributes and behaviours?\n\n\nShow the code\n# Load necessary libraries\nlibrary(ggplot2)\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nShow the code\nlibrary(corrplot)\n\n\nWarning: package 'corrplot' was built under R version 4.3.3\n\n\ncorrplot 0.92 loaded\n\n\nShow the code\n# Read the data\nhealth_data &lt;- read.csv(\"C:\\\\Users\\\\arava\\\\OneDrive\\\\Desktop\\\\STAT515\\\\modified_obesity_data.csv\")\n\n\n\n\nShow the code\n# Convert columns to appropriate data types if needed\nhealth_data$Age &lt;- as.numeric(as.character(health_data$Age))\nhealth_data$Height &lt;- as.numeric(as.character(health_data$Height))\nhealth_data$Weight &lt;- as.numeric(as.character(health_data$Weight))\n# Convert other columns to appropriate data types as needed...\n\n# Summary statistics\nsummary(health_data)\n\n\n    Gender               Age            Height          Weight      \n Length:2058        Min.   :14.00   Min.   :1.450   Min.   : 39.37  \n Class :character   1st Qu.:19.89   1st Qu.:1.630   1st Qu.: 66.00  \n Mode  :character   Median :22.73   Median :1.701   Median : 83.06  \n                    Mean   :24.05   Mean   :1.702   Mean   : 86.91  \n                    3rd Qu.:26.00   3rd Qu.:1.768   3rd Qu.:108.19  \n                    Max.   :47.28   Max.   :1.980   Max.   :173.00  \n family_history_with_overweight Highcaloric_food   Vegetable_intake\n Length:2058                    Length:2058        Min.   :1.000   \n Class :character               Class :character   1st Qu.:2.000   \n Mode  :character               Mode  :character   Median :2.390   \n                                                   Mean   :2.424   \n                                                   3rd Qu.:3.000   \n                                                   Max.   :3.000   \n Main_Meal_Intake Food_between_meals Smoking_Habit       Water_intake  \n Min.   :1.000    Length:2058        Length:2058        Min.   :1.000  \n 1st Qu.:2.681    Class :character   Class :character   1st Qu.:1.574  \n Median :3.000    Mode  :character   Mode  :character   Median :2.000  \n Mean   :2.694                                          Mean   :2.006  \n 3rd Qu.:3.000                                          3rd Qu.:2.473  \n Max.   :4.000                                          Max.   :3.000  \n Calorie_Intake     PHYSICAL_Activity Technology_dependency Alcohol_Consumption\n Length:2058        Min.   :0.0000    Min.   :0.0000        Length:2058        \n Class :character   1st Qu.:0.1205    1st Qu.:0.0000        Class :character   \n Mode  :character   Median :1.0000    Median :0.6287        Mode  :character   \n                    Mean   :0.9956    Mean   :0.6581                           \n                    3rd Qu.:1.6379    3rd Qu.:1.0000                           \n                    Max.   :3.0000    Max.   :2.0000                           \n Mode_of_Transportation Obesity_Levels    \n Length:2058            Length:2058       \n Class :character       Class :character  \n Mode  :character       Mode  :character  \n                                          \n                                          \n                                          \n\n\nShow the code\n# Histograms\npar(mfrow=c(3, 3))\nfor (i in 2:ncol(health_data)) {\n  if (is.numeric(health_data[, i])) {\n    hist(health_data[,i], main=names(health_data)[i], xlab=\"\")\n  }\n}\n\n# Box plots\npar(mfrow=c(3, 3))\n\n\n\n\n\nShow the code\nfor (i in 2:ncol(health_data)) {\n  if (is.numeric(health_data[, i])) {\n    boxplot(health_data[,i] ~ health_data$Obesity_Levels, main=names(health_data)[i], xlab=\"Obesity Levels\", ylab=names(health_data)[i])\n  }\n}\n\n# Scatter plot matrix\n# Filter numeric columns\nnumeric_cols &lt;- sapply(health_data, is.numeric)\nnumeric_data &lt;- health_data[, numeric_cols]\n\n# Scatter plot matrix\npairs(numeric_data, pch = 16)\n\n\n\n\n\n\n\n\nThis code is designed for exploratory data analysis (EDA) of a dataset related to health and obesity. It aims to provide insights into the characteristics of the dataset, including summary statistics, distributions of variables, relationships between variables, and potential patterns or trends.\nFirst, we will load two libraries - ggplot 2 for creating graphical visualizations and corrplot for plotting the correlation matrices. \nThen we will read the dataset from a csv file named obesity data and we have stored it in a variable named health data. \nConvert specific columns such as age, weight, and height to numeric data types.  It first coerces them into character type and then converts them into numeric type. This is often necessary when R reads these values as factors or characters instead of numeric.\nNow let’s get the summary statistics for the dataset including minimum, maximum, mean, median, 1st quartile and 3rd quartile values for the numeric columns. \nCreate histograms for each numeric variable in the dataset. It plots them in a 3x3 grid using the par function, iterating over each numeric column, and plotting a histogram for each one. \nCreate the box plots for each numeric variable grouped by ‘Obesity Levels’.  It also arranges them in a 3x3 grid similar to the histograms. \nCreate a scatter plot matrix for all the numeric variables in the dataset. It first filters out only numeric columns, stores them in numeric data, and then creates a scatter plot matrix using the pairs function, with points represented by filled circles (pch = 16).\n\n\nShow the code\nhealth_data &lt;- read.csv(\"C:\\\\Users\\\\arava\\\\OneDrive\\\\Desktop\\\\STAT515\\\\modified_obesity_data.csv\")\n\n# Select relevant features\nselected_features &lt;- health_data[, c(\"Age\", \"Height\", \"Weight\", \"family_history_with_overweight\", \"Highcaloric_food\", \n                                     \"Vegetable_intake\", \"Main_Meal_Intake\", \"Food_between_meals\", \"Smoking_Habit\", \n                                     \"Water_intake\", \"Calorie_Intake\", \"PHYSICAL_Activity\", \"Technology_dependency\", \n                                     \"Alcohol_Consumption\", \"Mode_of_Transportation\")]\n\n# Check for missing values and handle if necessary\n# For example: selected_features &lt;- na.omit(selected_features)\n\n# Perform feature scaling or normalization\n# Here, we'll use z-score standardization\n# Exclude non-numeric columns before scaling\nnumeric_features &lt;- selected_features[, sapply(selected_features, is.numeric)]\n\n# Perform feature scaling\nscaled_features &lt;- scale(numeric_features)\n\n\nThe purpose of this code is to prepare the dataset for further analysis or modeling by selecting relevant features, handling missing values if necessary, and standardizing numeric features through z-score standardization. These preprocessing steps are essential for ensuring the quality and consistency of the data before performing any downstream tasks such as modeling or statistical analysis.\n\n\nShow the code\nlibrary(cluster)\nlibrary(factoextra)\n\n\nWarning: package 'factoextra' was built under R version 4.3.3\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\n\nShow the code\n# Read the data\nhealth_data &lt;- read.csv(\"C:\\\\Users\\\\arava\\\\OneDrive\\\\Desktop\\\\STAT515\\\\modified_obesity_data.csv\")  \n\n# Select relevant features for clustering\nselected_features &lt;- health_data[, c(\"Age\", \"Height\", \"Weight\", \"family_history_with_overweight\", \"Highcaloric_food\", \n                                     \"Vegetable_intake\", \"Main_Meal_Intake\", \"Food_between_meals\", \"Smoking_Habit\", \n                                     \"Water_intake\", \"Calorie_Intake\", \"PHYSICAL_Activity\", \"Technology_dependency\", \n                                     \"Alcohol_Consumption\", \"Mode_of_Transportation\")]\n\n\n# Convert categorical variables to factors if needed\n# Convert categorical variables to dummy variables\nselected_features &lt;- model.matrix(~ . - 1, data = selected_features)\n\n# Perform K-means clustering\nk=3\nkmeans_result &lt;- kmeans(selected_features, centers = k)\n\n\n# Handle missing values if any\n# Example: selected_features &lt;- na.omit(selected_features)\n\n# Perform K-means clustering\nk &lt;- 3  # Number of clusters\nset.seed(123)  # Set seed for reproducibility\nkmeans_result &lt;- kmeans(selected_features, centers = k)\n\n\n# Adjust plot parameters for better visibility\nfviz_cluster(kmeans_result, \n             geom = \"point\", \n             data = selected_features, \n             stand = FALSE,  # Disable scaling for better visibility\n             ellipse.type = \"convex\",  # Use convex hulls for clusters\n             ellipse.level = 0.95,  # Confidence level for ellipses\n             palette = \"jco\",  # Color palette\n             ggtheme = theme_minimal() +  # Combine themes\n               theme(legend.position = \"bottom\"),  # Adjust legend position\n             main = paste(\"K-means Clustering (K =\", k, \")\"),  # Main title\n             show.clust.cent = TRUE,  # Show cluster centers\n             pointsize = 2)  # Adjust point size for better visibility\n\n\n\n\n\nShow the code\n# Evaluate clusters\nsilhouette_score &lt;- silhouette(kmeans_result$cluster, dist(selected_features))\n\n\nThis segment performs k-means clustering on the health and obesity dataset, visualizes the resulting clusters, and evaluates their quality using the silhouette score. Here is the break down of step by step process.\nFirstly, we will import the required libraries for performing clustering (cluster) and visualizing clustering results (factoextra).\nRead the dataset from a CSV file and stores it in the variable health_data.\nSelect specific features from the dataset that are deemed relevant for clustering. These features will be used as input variables for the k-means clustering algorithm\nConvert categorical variables into dummy variables, which are required for clustering algorithms like k-means that operate on numerical data.\nPerform k-means clustering on the selected features. It specifies the number of clusters (k), sets a seed for reproducibility, and then executes the k-means algorithm.\nVisualize the clusters obtained from the k-means algorithm. It plots the data points, cluster centers, and cluster ellipses for better interpretation of the clusters.\nCalculate the silhouette score to evaluate the quality of the clusters. The silhouette score provides a measure of how well-separated the clusters are and helps in assessing the effectiveness of the clustering algorithm.\nCluster Interpretation:\nBased on the clustering results, we can observe distinct groups of individuals with different health profiles. The clusters can be interpreted as follows:\nCluster 1:\nIndividuals with relatively lower age, height, and weight, with mixed dietary habits and moderate physical activity levels.\nCluster 2:\nIndividuals with higher age, height, and weight, often having family history with overweight, consuming high-caloric food frequently, and exhibiting low physical activity levels.\nCluster 3:\nIndividuals with moderate age and height, but higher weight, frequently having family history with overweight, consuming high-caloric food frequently, and exhibiting high physical activity levels.\nConclusion:\nThe clustering analysis revealed distinct groups of individuals with varying health attributes and behaviors. Understanding these clusters can inform personalized interventions and public health policies aimed at promoting healthy lifestyles and preventing overweight and obesity."
  }
]